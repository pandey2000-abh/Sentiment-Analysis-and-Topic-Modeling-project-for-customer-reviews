{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVIFfvnv5l4u",
        "outputId": "bee21134-f374-4530-cafc-b3d474db399f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Id   ProductId          UserId                      ProfileName  \\\n",
            "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
            "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
            "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
            "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
            "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
            "\n",
            "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
            "0                     1                       1      5  1303862400   \n",
            "1                     0                       0      1  1346976000   \n",
            "2                     1                       1      4  1219017600   \n",
            "3                     3                       3      2  1307923200   \n",
            "4                     0                       0      5  1350777600   \n",
            "\n",
            "                 Summary                                               Text  \n",
            "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
            "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
            "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
            "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
            "4            Great taffy  Great taffy at a great price.  There was a wid...  \n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Reviews.csv\")\n",
        "\n",
        "# Display first few rows of the data\n",
        "print(data.head())\n",
        "\n",
        "# Convert Score into sentiment labels: 1-3 as Negative, 4-5 as Positive\n",
        "data['Sentiment'] = data['Score'].apply(lambda x: 1 if x > 3 else 0)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X = data['Text']\n",
        "y = data['Sentiment']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text data for traditional ML models using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRmJhHo750F8",
        "outputId": "832e5518-4bc9-4fd9-e136-a89e866a1bd9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Accuracy: 0.8972829863401677\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.69      0.74     24666\n",
            "           1       0.92      0.96      0.94     89025\n",
            "\n",
            "    accuracy                           0.90    113691\n",
            "   macro avg       0.86      0.82      0.84    113691\n",
            "weighted avg       0.89      0.90      0.89    113691\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression Model\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train_vec, y_train)\n",
        "lr_pred = lr_model.predict(X_test_vec)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, lr_pred))\n",
        "print(classification_report(y_test, lr_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZxHXIVU56PV",
        "outputId": "e8181230-e622-44fc-ace5-2cd0b87fb590"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Accuracy: 0.835633427448083\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.28      0.42     24666\n",
            "           1       0.83      0.99      0.90     89025\n",
            "\n",
            "    accuracy                           0.84    113691\n",
            "   macro avg       0.86      0.63      0.66    113691\n",
            "weighted avg       0.84      0.84      0.80    113691\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 354752, number of negative: 100011\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 38.577013 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 733959\n",
            "[LightGBM] [Info] Number of data points in the train set: 454763, number of used features: 5000\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.780081 -> initscore=1.266139\n",
            "[LightGBM] [Info] Start training from score 1.266139\n",
            "LightGBM Accuracy: 0.8802279863841465\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.57      0.67     24666\n",
            "           1       0.89      0.97      0.93     89025\n",
            "\n",
            "    accuracy                           0.88    113691\n",
            "   macro avg       0.86      0.77      0.80    113691\n",
            "weighted avg       0.88      0.88      0.87    113691\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries for Naive Bayes and LightGBM\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Naive Bayes Model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_vec, y_train)\n",
        "nb_pred = nb_model.predict(X_test_vec)\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, nb_pred))\n",
        "print(classification_report(y_test, nb_pred))\n",
        "\n",
        "# LightGBM Model\n",
        "lgbm_model = LGBMClassifier()\n",
        "lgbm_model.fit(X_train_vec, y_train)\n",
        "lgbm_pred = lgbm_model.predict(X_test_vec)\n",
        "print(\"LightGBM Accuracy:\", accuracy_score(y_test, lgbm_pred))\n",
        "print(classification_report(y_test, lgbm_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sKSXl8R6nIN",
        "outputId": "e48974ca-96be-4e6b-e38e-71394e121da0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Cross-Validation Accuracy: 0.8959589944237776\n",
            "MultinomialNB Cross-Validation Accuracy: 0.8328294087641129\n",
            "[LightGBM] [Info] Number of positive: 283802, number of negative: 80008\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 39.849087 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 733987\n",
            "[LightGBM] [Info] Number of data points in the train set: 363810, number of used features: 5000\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.780083 -> initscore=1.266150\n",
            "[LightGBM] [Info] Start training from score 1.266150\n",
            "[LightGBM] [Info] Number of positive: 283801, number of negative: 80009\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 42.711156 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 733983\n",
            "[LightGBM] [Info] Number of data points in the train set: 363810, number of used features: 5000\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.780080 -> initscore=1.266134\n",
            "[LightGBM] [Info] Start training from score 1.266134\n",
            "[LightGBM] [Info] Number of positive: 283801, number of negative: 80009\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 37.302710 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 733434\n",
            "[LightGBM] [Info] Number of data points in the train set: 363810, number of used features: 5000\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.780080 -> initscore=1.266134\n",
            "[LightGBM] [Info] Start training from score 1.266134\n",
            "[LightGBM] [Info] Number of positive: 283802, number of negative: 80009\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 40.000814 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 734395\n",
            "[LightGBM] [Info] Number of data points in the train set: 363811, number of used features: 5000\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.780081 -> initscore=1.266138\n",
            "[LightGBM] [Info] Start training from score 1.266138\n",
            "[LightGBM] [Info] Number of positive: 283802, number of negative: 80009\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 32.475872 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 733614\n",
            "[LightGBM] [Info] Number of data points in the train set: 363811, number of used features: 5000\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.780081 -> initscore=1.266138\n",
            "[LightGBM] [Info] Start training from score 1.266138\n",
            "LGBMClassifier Cross-Validation Accuracy: 0.8782926547968227\n"
          ]
        }
      ],
      "source": [
        "# Implement k-fold cross-validation\n",
        "for model, name in zip([lr_model, nb_model , lgbm_model], ['Logistic Regression', 'MultinomialNB', 'LGBMClassifier']):\n",
        "    scores = cross_val_score(model, X_train_vec, y_train, cv=5)\n",
        "    print(f\"{name} Cross-Validation Accuracy: {np.mean(scores)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBcAjZKL7ec2"
      },
      "outputs": [],
      "source": [
        "# Deep Learning Model - LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4IvZbe17yrf",
        "outputId": "587988b2-d4e7-4072-f0fe-5f7620e5970c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m7106/7106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3171s\u001b[0m 445ms/step - accuracy: 0.8819 - loss: 0.2884 - val_accuracy: 0.9214 - val_loss: 0.1944\n",
            "Epoch 2/2\n",
            "\u001b[1m7106/7106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3207s\u001b[0m 446ms/step - accuracy: 0.9321 - loss: 0.1758 - val_accuracy: 0.9346 - val_loss: 0.1698\n",
            "\u001b[1m3553/3553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 60ms/step\n",
            "LSTM Accuracy: 0.9345946468937735\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.83      0.85     24666\n",
            "           1       0.95      0.96      0.96     89025\n",
            "\n",
            "    accuracy                           0.93    113691\n",
            "   macro avg       0.91      0.90      0.90    113691\n",
            "weighted avg       0.93      0.93      0.93    113691\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Tokenization and Padding\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding the sequences\n",
        "max_length = 100  # maximum length of sequences\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "# Define the LSTM model\n",
        "lstm_model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=128, input_length=max_length),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Dropout(0.5),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "lstm_model.fit(X_train_pad, y_train, epochs=2, batch_size=64, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "lstm_pred = (lstm_model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "print(\"LSTM Accuracy:\", accuracy_score(y_test, lstm_pred))\n",
        "print(classification_report(y_test, lstm_pred))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}